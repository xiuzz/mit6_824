# 课程架构
## 课程详细信息
lectures(课程的视频)

papers(课程关联的论文)

exams(这个应该对我们这种互联网学习者关联不大)

labs(4个实验)

所有的内容都在官网都能找到：

http://nil.csail.mit.edu/6.824/2020/schedule.html

我看的带中文字幕的视频：

https://www.bilibili.com/video/BV1R7411t71W
# 分布式系统概论
一个分布式系统的构建是为了什么？
一些列计算机集群通过网络共同完成某个一系列连贯的任务，目前的主流目标是分布式计算，分布式存储，以及更普遍适用的系统(分布式构建者，能够很好的隐藏分布式细节，使普通人能像使用一般化的单机软件一样使用分布式系统)

一些在构建时要解决的问题:

1. parallelism(并行管理)

2. fault tolerance(容错)

3. physical(物理问题,尤其是网络的物理建设)

4. security/isolated(安全与隔离，主要是可信验证)

5. partial failure(局部失效)

为什么会有分布式系统？

最开始的设想来源于集中化的系统存储与计算性能问题不足然后提出的分布式系统概念。因此这里涉及到几个必要的点：

## 1.扩展性(scalability)
如果要做分布式系统那么最理想的情况是：1台电脑，具有1x存储空间，1x算力，那么n台电脑所连接起来的电脑就具有n倍存储空间，n倍算力。这就是分布式系统的扩展性。因此多于扩展性问题，如果构造一个巧妙的连接系统，或者说算法是必要的。

## 2.容错(fault tolerance)

容错的问题指向的是什么？允许我们的系统中出现错误。在一个成百上千的计算机组成的分布式系统中，它的稳定性是不可靠的，网络随时都可能出问题，一个交换机风扇坏了，导致过热瘫痪，就可能引来相当多的节点没办法工作；或者，随时都有可能有人把电缆踩坏了，导致某个节点断电，因此容错在这种不可控的系统中就相当重要了。

容错体现在什么地方呢：
### 2.1 可用性(availability)
这里的可用性指的是在系统发生错误后，仍能保证系统的功能完整可用，我个人觉得可用性应该是一个在后面的学习中会遇到的比较有挑战的问题吧。
### 2.2 可恢复性(recoverablitiy)
可恢复不难理解，当发生错误后，肯定得想办法解决和恢复系统的正常运行。对于分布式存储来说，可恢复性可以体现在使用nvs(非已失存储，如硬盘，ssd，闪存等等)，冗余备份（replication），英文这个词应该只是指复制，但是这里的复制指的将原本的数据进行备份，所以就翻译成冗余备份了。

## 3.一致性（consistency）
这一部分给我看睡着了，主要这里他讲的太快了，让我一下子不能明白什么是强一致性，什么是弱一致性，后面看了些文档才理解了。

首先什么是一致性，一致性指的是前面的冗余复制，及保证多节点所备份的数据是一样的。然后是他举例说明了什么是强弱一致性。

### 3.1强一致性
两个要求：
1. 任何一次读的某个数据位置，都是这个位置上最新的数据。
2. 系统的所有进程看到的操作顺序，都和全局时钟下的顺序一次。(这一点可能理解起来有点抽象，我下面解释)。

这两点保证了任何时刻，任意节点在任何时刻（这里的时刻指的是对外提供服务的时刻）的数据都是一样的。它的模型是这样的：一个分布式集群里面，如果此时来了一个写操作，ok，这个节点向其他节点发送请求，让它们暂停服务，所有节点写入这段数据，这样就满足了刚刚说的第二点，全局时钟下的顺序一致，有点像啥原子操作了(事务要么同时成功，要么同时失败)。

当然这个模型是网上给的，是否是强一致性的标准我也不知道，不过强一致性就一句话任意节点在任何时刻（这里的时刻指的是对外提供服务的时刻）的数据都是一样的。当然强一致性势必会带来不少的性能损耗。

### 3.2弱一致性
弱一致性没有什么好说的，即当某个数据被更新后，不一定要马上保证所有的备份数据同步更新，用户在读的时候有可能是读到的旧的数据，也有可能读到的是新的数据。简而言之如果能容忍后续的访问到部分或者全部访问不到，则是弱一致性。现代的分布式系统，大多都研究和基于弱一致性，毕竟大多数分布式系统更多的目的是并行计算和存储，效率是关键。


# MapReduce

一个经典的分布式计算模型用于大规模数据计算

http://nil.csail.mit.edu/6.824/2020/papers/mapreduce.pdf

MapReduce本身只是一个模型，实际的代码实现细节会因人而异。MapReduce的细节等我看完论文再写。

这是一个分布式系统的MapReduce构建模型，这个模型也是lab1里面要构建的代码的模型所以就挑这个来说了
![Alt text](picture/a%20model%20of%20mapredcue.png)

步骤顺序与上面的编号顺序一致
## 步骤1 fork
首先假设有一个用户程序，然后我们这堆代码是一个写好的mapreduce库，用户调用mapreduce,传递三个参数：用户定义规则的map函数，用户定义规则的reduce规则的reduce函数，输入文件。然后该代码会将输入文件分成M片，每片的长度在16MB到64MB（用户可以通过可选的参数控制），然后它在分布式集群中启动(fork)节点，运行用户自定义的规则。

## 步骤2 assign map/reduce
启动的有一个节点是特殊的-->master节点。
其他启动的节点都是worker节点，他们的具体行为由主节点控制，因为输入为M个分区，输出为R个分区，因此有M个map任务和R个reduce任务要分配。master挑选空闲的worker,并为他分配一个map任务或者一个reduce任务。

## 步骤3 read
当worker被分配到一个map任务后，读取对应的分区，它从输入数据中解析key/value，并将每对传递给用户定义的map函数中。map函数生成中间键缓存在内存中。

## 步骤4 local write

缓存的k/v会定期写入本地磁盘，并由分区函数分在制定的一块R分区中(hash然后对R取模，很简单的思想),然后他们在本地磁盘的位置被传回主节点，主节点负责将这些位置传发给reduce工作节点。

## 步骤5 remote read

做好map后，master会通知一个节点执行Reduce，他会发送给这个map的位置信息，文件名，然后Reduce拿到位置信息，使用rpc(remote procedure calls)远程过程调用从Map Worker的本地磁盘中间数据k/v。当Reduce Worker读取所有中间数据时，它会按key的字典序进行排序，以便合并相同key（双指针）。如果中间数据过多，内存无法容纳，则使用外部排序。

## 步骤6 write

Reduce Worker 迭代排序后的中间数据，对于遇到的每个不重复（unique）的key,他将k/v传递给reduce函数，reduce输出到该分区的最终输出文件中。

## final

当所有map任务和reduce任务完成后，master唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码。成功完成后，mapreduce 执行的输出可在 R 输出文件中获得（每个reduce 任务一个，文件名由用户指定）。通常，用户不需要将这些 R 输出文件合并到一个文件中——他们通常将这些文件作为输入传递给另一个 MapReduce 调用，或者从另一个能够处理划分为多个文件的输入的分布式应用程序中使用它们。



